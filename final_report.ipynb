{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Group Project\n",
    "### Alberto Puentes, Parker Voit, Tyler Applegate\n",
    "#### Florence Cohort, 2021_08_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: expecting '}' (explore.py, line 94)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/opt/homebrew/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3418\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-de7e9c0121fb>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import explore\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/thxmanu/NLP_group_project/explore.py\"\u001b[0;36m, line \u001b[0;32m94\u001b[0m\n\u001b[0;31m    plt.title(f'{title_names[i]', font = 'Arial', fontsize= 20)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: expecting '}'\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from markdown import markdown\n",
    "import nltk\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "# visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from pprint import pprint\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# our function modules\n",
    "import acquire\n",
    "import prepare\n",
    "import explore\n",
    "\n",
    "# imports for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score, plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings:\n",
    "- The goal of this project was to try and identify which programming language was used in a GitHub repository by scraping their respective readme.md files and constructing a classification model that would 'accurately' predict the programming language.\n",
    "- We found that our K-Nearest Neighbors model outperformed all other models, including the baseline. \n",
    "- This model achieved 84% overall accuracy on our training data, 60% on our validate set, and finally 42% on the unseen test data set.\n",
    "- With more time, we would like to gather more data, explore more methods for feature engineering new features for natural language processing, and use some more robust deep learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our function to pull in raw data\n",
    "repo_df = acquire.get_github_data(cached = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquire.overview(repo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition - Key Findings & Takeaways:\n",
    "- We are looking at 120 observations, evenly distributed across 4 programming languages\n",
    "- There appears to be 1 null value\n",
    "- There are also multiple files that are in foreign languages\n",
    "- For our first iteration, we are going to drop the null value, and foreign language repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform our inital data prep and look at key stats\n",
    "repo_clean = prepare.initial_repo_prep(repo_df, 'readme_contents', extra_words = ['fr','freesvghttpsgithubcomlisadziubaawesomedesigntoolsblobmastermediafreesvg','opensourcesvghttpsgithubcomlisadziubaawesomedesigntoolsblobmastermediaopensourcesvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution of target variable\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(aspect=\"equal\"))\n",
    "cs_customer= cm.get_cmap('viridis')(np.linspace(0, 1, 5))\n",
    "language = ['Shell', 'Python', 'JavaScript', 'HTML']\n",
    "data = [29,27,24,22]\n",
    "wedges, texts = ax.pie(data, wedgeprops=dict(width=0.4), startangle=90,colors=cs_customer)\n",
    "plt.legend(wedges, language, loc=\"center\",fontsize=12)\n",
    "ax.set_title(\"Language Distribution\", color=\"white\", fontdict={'fontsize': 16, 'fontweight': 'bold'})\n",
    "fig.tight_layout()\n",
    "plt.savefig('donutplot2.png',dpi=100, format='png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our data prior to exploration, stratifying on target variable 'language'\n",
    "train, validate, test = prepare.train_validate_test_split(repo_clean, 'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does our training set look like?\n",
    "acquire.overview(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation - Key Findings & Takeaways:\n",
    "- We dropped 1 null observation, and 17 foreign language repos\n",
    "- We also created new columns of 'cleaned', 'stemmed', and 'lemmatized' data\n",
    "- Due to dropping the foreign language repos, our distribution is no longer uniform across the 4 programming languages\n",
    "- Instead of 30 each, we now have:\n",
    "    - 29 Shell\n",
    "    - 27 Python\n",
    "    - 24 JavaScript\n",
    "    - 22 HTML\n",
    "- We are hopeful that this imbalance will not impact our modeling too much\n",
    "- If it does, we will end up having to pull in more data\n",
    "- Data has been split into train, validate, and test sets that have been stratified on 'language' to keep as much balance as possible to each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of all words, java words, html words, python words, and shell words\n",
    "all_words = (' '.join(train.cleaned_readme_contents))\n",
    "java_words = (' '.join(train[train.language == 'JavaScript'].cleaned_readme_contents))\n",
    "shell_words = (' '.join(train[train.language == 'Shell'].cleaned_readme_contents))\n",
    "python_words = (' '.join(train[train.language == 'Python'].cleaned_readme_contents))\n",
    "html_words = (' '.join(train[train.language == 'HTML'].cleaned_readme_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the lists to number counts of each word\n",
    "all_freq = pd.Series(all_words.split()).value_counts()\n",
    "java_freq = pd.Series(java_words.split()).value_counts()\n",
    "shell_freq = pd.Series(shell_words.split()).value_counts()\n",
    "python_freq = pd.Series(python_words.split()).value_counts()\n",
    "html_freq = pd.Series(html_words.split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to save words to be parsed later\n",
    "documents = {'java': java_words,\n",
    "            'python': python_words,\n",
    "            'HTML': html_words,\n",
    "            'shell': shell_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple way to calculate idf for demonstration. Note that this\n",
    "# function relies on the globally defined documents variable.\n",
    "def idf(word):\n",
    "    n_occurences = sum([1 for doc in documents.values() if word in doc])\n",
    "    return len(documents) / n_occurences\n",
    "\n",
    "# Get a list of the unique words\n",
    "unique_words = pd.Series(' '.join(documents.values()).split()).unique()\n",
    "\n",
    "# put the unique words into a data frame\n",
    "dfx = (pd.DataFrame(dict(word=unique_words))\n",
    " # calculate the idf for each word\n",
    " .assign(idf=lambda df: df.word.apply(idf))\n",
    " # sort the data for presentation purposes\n",
    " .set_index('word')\n",
    " .sort_values(by='idf', ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = dfx[dfx.idf<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_clean = prepare.second_repo_prep(repo_df, 'readme_contents', extra_words = stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = prepare.train_validate_test_split(repo_clean, 'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_list = [all_words, java_words, shell_words, python_words, html_words]\n",
    "that_list = ['all_words', 'java_words', 'shell_words', 'python_words', 'html_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the top 10 words for each language\n",
    "explore.word_count_word_cloud(this_list, that_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at bigrams\n",
    "explore.bigram_word_cloud(this_list, that_list, n = 2, x=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally, let's see trigrams...\n",
    "explore.trigram_word_cloud(this_list, that_list, n = 3, x=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['length'] = [len(i) for i in train.cleaned_readme_contents]\n",
    "train.groupby(by = 'language').mean().round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration - Key Findings & Takeaways:\n",
    "- Most Common Single Words:\n",
    "    - Quite a bit of overlap between languages\n",
    "    - Each language appears to have a few words that are exclusive to their top used list\n",
    "    - Looks like we need to drop some of the most common words across all languages, to get more value out of single words\n",
    "- Most Common Bigrams:\n",
    "    - Less overlap than on single words, but still a little too much\n",
    "    - We are starting to see a different vocabulary for each programming language emerge\n",
    "    - Will get more value after dropping most common words across all languages\n",
    "- Most Common Trigrams:\n",
    "    - Very interesting to see how many trigrams are made up of overlapping bigrams\n",
    "    - Least amount of overlap here, and interesting to see how many words appear in multiple trigrams for a language\n",
    "- Length Takeaways:\n",
    "    - Python has the largest average character count \n",
    "    - JavaScript and Shell have somewhat equal character counts\n",
    "    - HTML has the least amount\n",
    "    - There is an imbalance here that ideally would be addressed with more time, to avoid over-fitting of models\n",
    "- With More Time:\n",
    "    - Explore various feature engineering, such as bigrams, trigrams, length of document, and sentiment analysis\n",
    "    - Experiment with additional / fewer stopwords\n",
    "    - Gather more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our X variables\n",
    "X_train = train.lemmatized_readme_contents\n",
    "X_validate = validate.lemmatized_readme_contents\n",
    "X_test = test.lemmatized_readme_contents\n",
    "\n",
    "# Setup our y variables\n",
    "y_train = train.language\n",
    "y_validate = validate.language\n",
    "y_test = test.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.language.value_counts()\n",
    "# establish a baseline using the most common value\n",
    "train['baseline'] = 'Shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = (train.baseline == y_train).mean()\n",
    "print(f'Baseline accuracy is {baseline * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tfidf vectorizer object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit the object on the training data\n",
    "tfidf.fit(X_train)\n",
    "\n",
    "# Save as vectorized object\n",
    "X_train_vectorized = tfidf.transform(X_train)\n",
    "X_validate_vectorized = tfidf.transform(X_validate) \n",
    "X_test_vectorized = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the RF object\n",
    "rf = RandomForestClassifier(bootstrap=True, \n",
    "                            class_weight=None, \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=20,\n",
    "                            max_depth=5, \n",
    "                            random_state=123)\n",
    "\n",
    "#Fit the RF object to the training data\n",
    "rf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "#Predict on y\n",
    "y_pred = rf.predict(X_train_vectorized)\n",
    "rf.score(X_train_vectorized, y_train)\n",
    "#Evaluate\n",
    "print(classification_report(y_train, y_pred, zero_division = 0))\n",
    "print(f'Model accuracy is {rf.score(X_train_vectorized, y_train)*100:.2f}%')\n",
    "print(f'Baseline accuracy is {baseline * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_validate_vectorized)\n",
    "print(classification_report(y_validate, y_pred, zero_division=0))\n",
    "print(f'Model accuracy is {rf.score(X_validate_vectorized, y_validate)*100:.2f}%')\n",
    "print(f'Baseline accuracy is {baseline * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the CLF object\n",
    "clf = DecisionTreeClassifier(max_depth=4, random_state=123, min_samples_leaf = 10)\n",
    "\n",
    "#Fit the model on the training set \n",
    "clf = clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "#Make predictions\n",
    "y_pred = clf.predict(X_train_vectorized)\n",
    "\n",
    "#Evaluate model performance on training data\n",
    "print(classification_report(y_train, y_pred,zero_division=0))\n",
    "print(f'Model accuracy is {clf.score(X_train_vectorized, y_train)*100:.2f}%')\n",
    "print(f'Baseline accuracy is {baseline * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_validate_vectorized)\n",
    "print(classification_report(y_validate, y_pred, zero_division=0))\n",
    "print(f'Model accuracy is {clf.score(X_validate_vectorized, y_validate)*100:.2f}%')\n",
    "print(f'Baseline accuracy is {baseline * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the naive bayes object\n",
    "nb = MultinomialNB()\n",
    "\n",
    "#Fit the model on the training set \n",
    "nb = nb.fit(X_train_vectorized, y_train)\n",
    "\n",
    "#Make predictions\n",
    "y_pred = nb.predict(X_train_vectorized)\n",
    "\n",
    "#Evaluate model performance on training data\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(f'Model accuracy is {nb.score(X_train_vectorized, y_train)*100:.2f}%')\n",
    "print(f'Baseline accuracy is {baseline * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "y_pred = nb.predict(X_validate_vectorized)\n",
    "\n",
    "#Evaluate model performance on training data\n",
    "print(classification_report(y_validate, y_pred))\n",
    "print(f'Model accuracy is {nb.score(X_validate_vectorized, y_validate)*100:.2f}%')\n",
    "print(f'Baseline accuracy is {baseline * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create KNN classifier object\n",
    "knn = KNeighborsClassifier(n_neighbors=2, weights='uniform')\n",
    "\n",
    "# fit the model to vectorized data\n",
    "knn.fit(X_train_vectorized, y_train)\n",
    "y_pred = knn.predict(X_train_vectorized)\n",
    "\n",
    "\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(f'Model accuracy is {knn.score(X_train_vectorized, y_train)*100:.2f}%')\n",
    "print(f'Baseline accuracy is {baseline * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_validate_vectorized)\n",
    "print(classification_report(y_validate, y_pred, zero_division=0))\n",
    "print(f'Model accuracy is {knn.score(X_validate_vectorized, y_validate)*100:.2f}%')\n",
    "print(f'Baseline accuracy is {baseline * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building - Key Findings & Takeaways:\n",
    "\n",
    "- All models passed the baseline on the validate split, except for the Random Forest Classifier\n",
    "- All models are overfit\n",
    "- K-Nearest Neighbors performed the best, outperforming the baseline by 31.43%\n",
    "    - Model efficacy could be improved by increasing the amount of readmes in the dataset\n",
    "    - Readme length also greatly varied across languages which creates a psuedo-class imbalance. Finding a method to scale readme length would help reduce overfitting\n",
    "    - Further hyper-parameter tuning and feature engineering could help generalize the model better\n",
    "- Most were pretty imbalanced when predicting all four languages, but K-Nearest Neighbors predicted all but shell somewhat equally\n",
    "- For these reasons, we will use KNN on the test set\n",
    "- With More Time:\n",
    "    - All models would benefit greatly from an increased sample size\n",
    "    - We found that distance-based models outperformed decision-based models, and would like to continue adjusting hyperparameters\n",
    "    - We would also like to look into clustering, and combining models to increase accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test_vectorized)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Model accuracy on test (unseen data) is {knn.score(X_test_vectorized, y_test)*100:.2f}%')\n",
    "print(f'Baseline accuracy is {baseline * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation - Key Findings & Takeaways:\n",
    "- The model outperformed the baseline by 15% and was not too lopsided in predicting each language\n",
    "- The model performed very well in predicting HTML code\n",
    "- Performs the worst with Shell\n",
    "- More data is needed to increase K and reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle it for later \n",
    "filename = 'knn_github_model.sav'\n",
    "pickle.dump(knn, open(filename, 'wb'))\n",
    " \n",
    "# load the model from disk to test\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test_vectorized, y_test)\n",
    "print(f'Model accuracy score on input data: {result:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusions & Next Steps: \n",
    "- Conclusions:\n",
    "    - It is possible to predict coding languages based off of readme files alone\n",
    "    - Special care needs to be taken during the cleaning phase of the pipeline as the quality of words fed into the algorithm is crucial to model performance\n",
    "    - Domain knowledge is important for determining important stop words \n",
    "    - TF-IDF vectorizing worked better for this data\n",
    "    - More data could help with overfitting, or possibly addressing the imbalanced character counts for each language's readme.\n",
    "\n",
    "- Next Steps \n",
    "    - All models would benefit greatly from an increased sample size\n",
    "    - We found that distance-based models outperformed decision-based models, and would like to continue adjusting hyperparameters\n",
    "    - We would also like to look into clustering, and combining models to increase accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
